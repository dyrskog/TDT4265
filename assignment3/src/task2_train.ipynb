{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple example on how you can use a jupyter notebook to train your model :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joerg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataloaders import load_cifar10\n",
    "from trainer import Trainer, compute_loss_and_accuracy\n",
    "from task2 import create_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_channels,\n",
    "                 num_classes):\n",
    "        \"\"\"\n",
    "            Is called when model is initialized.\n",
    "            Args:\n",
    "                image_channels. Number of color channels in image (3)\n",
    "                num_classes: Number of classes we want to predict (10)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Implement this function (Task  2a)\n",
    "        num_filters = 32  # Set number of filters in first conv layer\n",
    "        self.num_classes = num_classes\n",
    "        # Define the convolutional layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=image_channels,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "            )\n",
    "        )\n",
    "        # The output of feature_extractor will be [batch_size, num_filters, 16, 16]\n",
    "        self.num_output_features = 4*4*128\n",
    "        # Initialize our last fully connected layer\n",
    "        # Inputs all extracted features from the convolutional layers\n",
    "        # Outputs num_classes predictions, 1 for each class.\n",
    "        # There is no need for softmax activation function, as this is\n",
    "        # included with nn.CrossEntropyLoss\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1, end_dim=3), # Flatten\n",
    "            nn.Linear(self.num_output_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model\n",
    "        Args:\n",
    "            x: Input image, shape: [batch_size, 3, 32, 32]\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function (Task  2a)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        features = self.feature_extractor(x)\n",
    "        out = self.classifier(features)\n",
    "\n",
    "        expected_shape = (batch_size, self.num_classes)\n",
    "        assert out.shape == (batch_size, self.num_classes),\\\n",
    "            f\"Expected output of forward pass to be: {expected_shape}, but got: {out.shape}\"\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "ExampleModel(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=3)\n",
      "    (1): Linear(in_features=2048, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch: 0, Batches per seconds: 27.61, Global step:    351, Validation Loss: 1.62, Validation Accuracy: 0.397\n",
      "Epoch: 0, Batches per seconds: 33.47, Global step:    702, Validation Loss: 1.41, Validation Accuracy: 0.493\n",
      "Epoch: 1, Batches per seconds: 31.46, Global step:   1053, Validation Loss: 1.33, Validation Accuracy: 0.517\n",
      "Epoch: 1, Batches per seconds: 33.82, Global step:   1404, Validation Loss: 1.18, Validation Accuracy: 0.582\n",
      "Epoch: 2, Batches per seconds: 32.76, Global step:   1755, Validation Loss: 1.10, Validation Accuracy: 0.619\n",
      "Epoch: 2, Batches per seconds: 34.42, Global step:   2106, Validation Loss: 1.04, Validation Accuracy: 0.636\n",
      "Epoch: 3, Batches per seconds: 33.64, Global step:   2457, Validation Loss: 1.00, Validation Accuracy: 0.656\n",
      "Epoch: 3, Batches per seconds: 34.80, Global step:   2808, Validation Loss: 0.91, Validation Accuracy: 0.684\n",
      "Epoch: 4, Batches per seconds: 34.11, Global step:   3159, Validation Loss: 0.91, Validation Accuracy: 0.682\n",
      "Epoch: 4, Batches per seconds: 35.05, Global step:   3510, Validation Loss: 0.82, Validation Accuracy: 0.711\n",
      "Epoch: 5, Batches per seconds: 34.44, Global step:   3861, Validation Loss: 0.88, Validation Accuracy: 0.702\n",
      "Epoch: 5, Batches per seconds: 35.17, Global step:   4212, Validation Loss: 0.81, Validation Accuracy: 0.712\n",
      "Epoch: 6, Batches per seconds: 34.51, Global step:   4563, Validation Loss: 0.81, Validation Accuracy: 0.731\n",
      "Epoch: 6, Batches per seconds: 35.15, Global step:   4914, Validation Loss: 0.79, Validation Accuracy: 0.733\n",
      "Epoch: 7, Batches per seconds: 34.58, Global step:   5265, Validation Loss: 0.86, Validation Accuracy: 0.727\n",
      "Epoch: 7, Batches per seconds: 35.13, Global step:   5616, Validation Loss: 0.86, Validation Accuracy: 0.720\n",
      "Epoch: 8, Batches per seconds: 34.74, Global step:   5967, Validation Loss: 0.96, Validation Accuracy: 0.712\n",
      "Early stop criteria met\n",
      "Early stopping.\n",
      "Train Accuracy: tensor(0.8789)\n",
      "Test Accuracy: tensor(0.7272)\n",
      "Validation Accuracy: tensor(0.7164)\n",
      "Train Loss: tensor(0.3460)\n",
      "Test Loss: tensor(0.8232)\n",
      "Validation Loss: tensor(0.9438)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 5e-2\n",
    "early_stop_count = 4\n",
    "dataloaders = load_cifar10(batch_size)\n",
    "model = ExampleModel(image_channels=3, num_classes=10)\n",
    "trainer = Trainer(\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    early_stop_count,\n",
    "    epochs,\n",
    "    model,\n",
    "    dataloaders\n",
    ")\n",
    "trainer.train()\n",
    "final_train_loss , final_train_accuracy = compute_loss_and_accuracy(dataloaders[0], trainer.model, trainer.loss_criterion)\n",
    "final_val_loss , final_val_accuracy = compute_loss_and_accuracy(dataloaders[1], trainer.model, trainer.loss_criterion)\n",
    "final_test_loss , final_test_accuracy = trainer.model_test()\n",
    "\n",
    "print(\"Train Accuracy: \" + str(final_train_accuracy))\n",
    "print(\"Test Accuracy: \" + str(final_test_accuracy))\n",
    "print(\"Validation Accuracy: \" + str(final_val_accuracy))\n",
    "\n",
    "print(\"Train Loss: \" + str(final_train_loss))\n",
    "print(\"Test Loss: \" + str(final_test_loss))\n",
    "print(\"Validation Loss: \" + str(final_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(trainer, \"task2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c4fcee83d90034a31e2dba4cf23e6cd499b74dfaa582d524e6db637b53ec8d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
