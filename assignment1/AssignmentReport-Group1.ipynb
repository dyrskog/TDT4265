{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w_i}\n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C^n}{\\partial \\hat{y}} &= - \\frac{y^n}{\\hat{y}^n} - \\frac{1}{1 - \\hat{y}^n} \\cdot (-1) + \\frac{y^n}{1-\\hat{y}^n} \\cdot (-1) \\\\\n",
    "    &= \\frac{1 - y^n}{1 - \\hat{y}^n} - \\frac{y^n}{\\hat{y}^n} \\\\\n",
    "    \\frac{\\partial \\hat{y}}{\\partial w_i} &= \\frac{\\partial f(x^n)}{\\partial w_i} = x_i^n\\hat{y}^n(1-\\hat{y}^n)\n",
    "\\end{align*}\n",
    "\n",
    "Inserted in the original equation:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_i} &= \\left( \\frac{1 - y^n}{1 - \\hat{y}^n} - \\frac{y^n}{\\hat{y}^n} \\right)x_i^n\\hat{y}^n(1-\\hat{y}^n) \\\\\n",
    "    &= (1-y^n)x_i^n\\hat{y}^n - x_i^ny^n(1-\\hat{y}^n) \\\\\n",
    "    &= x_i^n\\hat{y^n} - x_i^ny^n \\\\\n",
    "    &= (\\hat{y}^n - y^n)x_i^n \\\\\n",
    "    &= -(y^n - \\hat{y}^n)x_i^n\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Use the rule $ln(\\frac{a}{b}) = ln(a) - ln(b)$ on the expression for $C^n(w)$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_{k,j}} &= -\\frac{\\partial}{\\partial w_{k,j}} \\sum_{p=1}^K y_p^n ln(\\hat{y}_p^n) \\\\\n",
    "    &= -\\frac{\\partial}{\\partial w_{k,j}} \\sum_{p=1}^K y_p^n \\left( ln(e^{z_p}) - ln(\\sum_{p'}^K e^{z_{p'}})\\right) \\\\\n",
    "    &= - \\sum_{p=1}^K \\frac{\\partial}{\\partial w_{k,j}} y_p^n z_p + \\sum_{p=1}^K \\frac{\\partial}{\\partial w_{k,j}} y_p^n ln \\left(\\sum_{p'}^K e^{z_{p'}}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Find the derivative for the first term, which can be split into the case of $k = p$ and $k \\neq p$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial w_{k,j}} y_p^n z_p &= \\frac{\\partial}{\\partial w_{k,j}} y_p^n \\left( w_p^T x \\right) \\\\\n",
    "    & = y_p^n \\frac{\\partial}{\\partial w_{k,j}} \\sum_j^I w_{p,j} \\cdot x_j = \\begin{cases}\n",
    "        0, \\quad k \\neq p \\\\\n",
    "        y_k^n x_j^n, \\quad k = p\n",
    "    \\end{cases}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Substituting in for the first term and using the chain rule on the last term, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_{k,j}} &= -y_k^n x_j^n + \\sum_{p=1}^K y_p^n \\frac{1}{\\sum_{p'}^K e^{z_{p'}}} \\frac{\\partial}{\\partial w_{k,j}}\\left(\\sum_{p'}^K e^{z_{p'}}\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Again, we can split the last term into two cases, $k = p'$ and $k \\neq p'$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial w_{k,j}} e^{z_{p'}} &= \\begin{cases}\n",
    "            0 &, \\quad k \\neq p' \\\\\n",
    "            e^{z_k} \\cdot \\frac{\\partial}{\\partial w_{k,j}} w_k x = e^{z_k} x_j &, \\quad k = p'\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_{k,j}} &= -y_k^n x_j^n + \\sum_{p=1}^K y_p^n \\frac{1}{\\sum_{p'}^K e^{z_{p'}}} e^{z_k^n} x_j^n\\\\\n",
    "    &= -y_k^n x_j^n + \\sum_{p=1}^K y_p^n \\hat{y}_k^n x_j^n \\\\\n",
    "    &= -y_k^n x_j^n + \\hat{y}_k^n x_j^n \\\\\n",
    "    &= -x_j^n(y_k^n - \\hat{y}_k^n)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](figures/task2b_binary_train_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](figures/task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "The early stopping kicks in at epoch 1038. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "\n",
    "Not sure about this one.\n",
    "\n",
    "\n",
    "![](figures/task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](figures/task3b_softmax_train_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](figures/task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "Yes, the training accuracy continues to rise, while validation accuracy does not improve. This indicates overfitting, as the model is only getting better at recognizing the training data instead of improving at identifying numbers in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Fill in image of hand-written notes which are easy to read, or latex equations here\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J(w)}{\\partial w} = \\frac{\\partial C(w)}{\\partial w} + \\lambda \\frac{\\partial R(w)}{\\partial w}    \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial R(w)}{\\partial w} = \\frac{\\partial}{\\partial w} \\sum_{i,j} w_{i,j}^2 = 2 \\sum_{i,j} w_{i,j} = 2w\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J(w)}{\\partial w} = \\frac{\\partial C(w)}{\\partial w} + 2 \\lambda w \n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "The weights get less noisy when introducing the term that penalizes a complex model. It makes sense that a complex, overtrained model will have very specific weights for each node leading to a more \"noisy\" visualization, whereas a less complex model will have a more \"smooth\" distribution between the weights. The less complex model will then pick up on more broad shapes and might be better outside of training.\n",
    "\n",
    "![](figures/task4b_softmax_weight.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](figures/task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "I think this is because we don't have a very complex model to begin with, and applying too much regularization might \"dumb down\" the model too much to work in a general example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "The norm increases with lower $\\lambda$-values.  \n",
    "\n",
    "![](figures/task4d_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c4fcee83d90034a31e2dba4cf23e6cd499b74dfaa582d524e6db637b53ec8d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
